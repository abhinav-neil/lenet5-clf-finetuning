\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}

\title{CV1 Final Project Part 2}
\author{Group 18}
\date{October 2022}

\begin{document}

\maketitle

\section{Image Classification on CIFAR-100}
\subsection{Explaining the dataset}
The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.

The CIFAR-100 dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs). \href{https://www.cs.toronto.edu/~kriz/cifar.html}{Source}

Some of the classes (0 to 4) are visualized in figure \ref{fig:cifar100-sample}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/cifar100-sample-1.png}
    \includegraphics[width=0.8\textwidth]{images/cifar100-sample-2.png}
    \includegraphics[width=0.8\textwidth]{images/cifar100-sample-3.png}
    \caption{Samples from classes 0 to 4 of CIFAR-100}
    \label{fig:cifar100-sample}
\end{figure}


\subsection{Architecture Understanding}
The first convolutional layer of ConvNet has a kernel size of 5.
The layer F6 is the first fully-connected layer with 120 input neurons and 84 output neurons, plus a bias. The number of learnable parameters (weights) in this layer is 120*84 + 84 = 10164.

\subsection{Preparation of Training}
We construct the Two Layer Net with input layer size= 3*32*32=3072 (=size of 1 image), hidden layer size = 64, output layer size = 100 (number of classes). We use ReLU as the activation function for the hidden layer and no activation function for the output layer. We use Cross Entropy Loss as the loss function. We use SGD as the optimizer with learning rate = 0.001, momentum = 0.9, weight decay = 0. We train the model for 20 epochs. We use the CIFAR-100 dataset for training and testing. We then evaluate the model on the test set and achieve an average accuracy of roughly 18\%. The training loss curve is plotted in figure \ref{fig:loss-tln}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/loss-tln.png}
    \caption{Training loss curve for Two Layer Net on CIFAR-100} 
    \label{fig:loss-tln}
\end{figure}

We construct the ConvNet using the architecture described \href{https://ieeexplore.ieee.org/document/726791}{LeNet-5} with input layer size= 3*32*32=3072 (=size of 1 image), output size (f7) = 100 (number of classes). We use $tanh$ as the activation function for the hidden layer and no activation function for the output layer. We use Cross Entropy Loss as the loss function. We use the AdAM optimizer with learning rate = 0.001, betas = 0.9 and 0.99, epsilon = $10^{-8}$ and weight decay = 0. We train the model for 20 epochs. We use the CIFAR-100 dataset for training and testing. We then evaluate the model on the test set and achieve an average accuracy of roughly 28\%. The training loss curve is plotted in figure \ref{fig:loss-cn}.

We notice that the ConvNet outperforms the TwoLayerNet in terms of test accuracy. This is expected as the network has more trainable parameters, and also convolutional nets are specialized for image classification tasks. However, it also takes longer to train as it is more complex and has more parameters to learn. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/loss-cn.png}
    \caption{Training loss curve for ConvNet on CIFAR-100} 
    \label{fig:loss-cn}
\end{figure}

\subsection{Setting up Hyperparameters}
\label{sec:hyperparameters}
First, we change the architecture of TwoLayerNet and ConvNet. We augment the existing network structures with BatchNorm layers. 
% A dropout layer randomly zeroes out a fraction $p$ of the neurons in the preceeding layer in each pass. This is a means of regularization as we prevent the network from overfitting. It prevents co-adaptations between neurons in the same layer which leads to learning false weights and overfitting. 

Batch Normalization is a form of regularization. This involves standardizing the weights in the layer (to zero mean and unit variance). This process greatly improves the speed and efficiency of the netowrk. It also addresses the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network.

For TwoLayerNet, we change the size of the first FC layer to 3072 (input size) $\times$ 64. This is followed by a ReLU layer, and then a Batch Norm layer of size 64. Next, we add a second FC layer of size 64 $\times$ 128. This is followed by the ReLU activation and a second batch-norm layer of size 128. Finally, we add a third FC layer of size 128 $\times$ 100 (output size). We use Cross Entropy Loss as the loss function. We use SGD as the optimizer with learning rate = 0.001, momentum = 0.9, weight decay = 0. We train the model for 20 epochs. We then evaluate the model on the test set and achieve an average accuracy of roughly 22\%, which is an improvement on the earlier version. The training loss curve is plotted in figure \ref{fig:loss-tln2}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/loss-tln2.png}
    \caption{Training loss curve for TwoLayerNet2 on CIFAR-100} 
    \label{fig:loss-tln2}
\end{figure}

For the ConvNet, we replace all the Tanh activations with ReLU units. We also replace the average pooling layers with max pooling. We change the size of the last convolional layer to 16 $\times$ 128 and the first FC layer (F6) to 128 $\times$ 64. We add a batch normalization layer of size 64 after the F6 layer. Finally, we change the output FC layer (F7) to 64 $\times$ 100. We use Cross Entropy Loss as the loss function. We use the AdAM optimizer with learning rate = 0.001, betas = 0.9 and 0.99, epsilon = $10^{-8}$ and weight decay = 0. We train the model for 20 epochs. We then evaluate the model on the test set and achieve an average accuracy of roughly 34\%, which is a large improvement on the previous version. The training loss curve is plotted in figure \ref{fig:loss-cn2}.

One important hyperparameter (for both models) was the batch size. Increasing the batch size (to 64) achieves a boost in efficiency without much deterioration in performance (in fact, accuracy actually increases). Increasing the size of the network (both width and depth) also improves performance as we have more parameters to learn. However, this also increases the training time, so we need to be careful about the tradeoff between performance and efficiency. The Stochastic Gradient Descent optimizer was the best choice for the TwoLayerNet, while the Adam optimizer was the best choice for the ConvNet. The SGD is a robust optimizer that works well for most problems. From browsing the literature, and through some trial and error on our training set, we found the following to be the best choice of hyperparameters for SGD: learning rate = 0.001, momentum = 0, weight decay = 0.  For the ConvNet, the AdAM optimizer performed best. The Adam optimizer is a more advanced optimizer that is more efficient than SGD. It is also more robust to hyperparameter tuning. We found the following to be the best choice of parameters for our use-case: learning rate = 0.001, betas = 0.9 and 0.99, epsilon = $10^{-8}$ and weight decay = 0. We won't describe the role of each parameter in depth as it is not very relevant here. However, the learning rate is quite important for both the optimizers. The large initial value causes the network to learn quickly, but it also causes the network to overshoot the minimum. The smaller value causes the network to learn slowly, but it also prevents the network from overshooting the minimum. We found that a learning rate of 0.001 achieved the best trade-off. Finally, adding batch normalization layers to the network also improved the performance of the networks. This is because batch normalization layers standardize the weights in the layer (to zero mean and unit variance). This process greatly improves the speed and efficiency of the netowrk. It also addresses the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network.

The TwoLayerNet has a generic architecture that can be used for any classification task. However, the ConvNet is specialized for image classification tasks. This is because of the presence of convolutional layers, followed by pooling layers. They are able to extract features from the image and learn the weights of the features. This is why the ConvNet outperforms the TwoLayerNet in terms of test accuracy. However, it also takes longer to train as it is more complex and has more parameters to learn.



\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/loss-cn2.png}
    \caption{Training loss curve for ConvNet2 on CIFAR-100} 
    \label{fig:loss-cn2}
\end{figure}

\section{Fine-tuning the ConvNet}

\subsection{STL-10 Dataset}
We download the STL10 dataset and extract a subset of 5 classes ('car', 'deer', 'horse', 'monkey', 'truck'), and then remap them to the new labels (0, 1, 2, 3, 4 respectively). 

\subsection{Fine-tuning}
We create a new ConvNet2 model as defined in section \ref{sec:hyperparameters}. We then load the weights of the ConvNet2 model trained in section \ref{sec:hyperparameters} into the ConvNet2 model. We then remove the final FC layer (thus discarding its weights) and add a new FC layer of size 64 $\times$ 5 (the STL dataset has 5 classes). We use Cross Entropy Loss as the loss function. We use the AdAM optimizer with learning rate = 0.001, betas = 0.9 and 0.99, epsilon = $10^{-8}$ and weight decay = 0. We train the model for 20 epochs. We then train the ConvNet2 model for 20 epochs. We use a subset of the STL-10 dataset for training and testing. We then evaluate the model on the test set and achieve an average accuracy of roughly 72\%. This is a quite high and indicates that pretraining and fine-tuning the network managed to achieve very good performanc with only a few epochs of training. The training loss curve is plotted in figure \ref{fig:loss-cn-stl}. We observe that there is a sharp decline in the training loss after the first epoch. This is because the weights of the ConvNet2 model are already trained and only need to be fine-tuned to the new dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/loss-cn-stl.png}
    \caption{Training loss curve for ConvNet on STL-10 subset} 
    \label{fig:loss-cn-stl}
\end{figure}

\end{document}